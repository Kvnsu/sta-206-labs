---
title: "206 final project"
author: "Kevin Su, Ran Ma, Sixue Cheng"
date: "11/23/2021"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "C:\\Users\\RanMa\\Desktop\\2021fall\\STA206\\final project")
knitr::opts_chunk$set(out.width = '90%', fig.align = 'center', echo = TRUE)
library(MASS)
library(ggplot2)
```

## Background

Abalone has long been farmed and harvested for numerous uses, notably as food for consumption and for pearls, which have a chance of being produced by their beautiful, iridescent inner shells made of a material known as “mother of pearl,” or nacre, which is also itself prized as a decorative material. 

The number of layers or rings within the shell of an
abalone serves as an effective proxy to the amount of usable nacre in the shell. These layers grow at regular intervals throughout the life of abalone so older abalone tend to have more usable and brilliant nacre. The age of abalone is determined by cutting the shell through the
cone, staining it, and counting the number of rings through a microscope – a time-consuming task. This project instead seeks to predict the number of rings of abalone (and hence its age) using its other more easily measurable features.

## Dataset

The dataset we used is provided by the UCI machine learning Repository called "Abalone".

```{r}
abalone_ori = read.table("abalone.txt",sep=",", header=FALSE)
length(abalone_ori); nrow(abalone_ori)
colnames(abalone_ori) = c("sex","length","diameter","height","whole_weight","shucked_weight","viscera_weight","shell_weight","rings")
sapply(abalone, class)
abalone_ori['sex'] = factor(abalone_ori$sex)
#age <- as.data.frame(abalone$rings+1.5)
#abalone['age'] <- age
head(abalone_ori)
```

The dataset contains 4177 observations of 9 characteristics of individual abalone. Of these 9 characteristics, only 1 variable is categorical(`sex`), indicating the sex of the abalone, all other variables are quantitive variables. 3 numeric variables correspond to the dimensions of the abalone (`length`,`diameter` and `height`), 4 numeric variables correspond to various weight measurements (`whole_weight`, `shucked_weight`, `viscera_weight` and `shell_weight`). The remaining variable is an integer(`rings`), indicating the number of rings the abalone has.

Note that the original documentation from UCI data repository indicates that continuous explanatory variables were divided by 200.


## Exploratory Data Analysis

```{r}
sapply(abalone_ori,function(x) {length(which(is.na(x)))})
table(duplicated(abalone_ori))
```

So there is no missing value or duplicated records in the dataset.  

```{r}
summary(abalone_ori)

par(mfrow = c(2,2))
colnum_quan = c(2:8)
for (i in colnum_quan) {
  box = boxplot(abalone_ori[,i],range = 2,horizontal = TRUE, col='steelblue', main = paste('box plot of', colnames(abalone_ori[i]), sep = ' '), xlab = colnames(abalone_ori[i]))}
```

```{r}
abalone_ori[which.max(abalone_ori$height),]
```

From the summary and the box plot of quantitative variables, we can see that the largest `height` is $1.1300\times 200$mm, which is nearly $10$ times of the upper quartile ($0.1650\times 200$mm).A 7 inch height (not length) abalone (1.13*200mm) is unreasonable when measured laying on its side, and all of its other measurements otherwise seem very ordinary. It could be a mistake in data collection or data entry. As such, we remove observation 2052.


```{r}
abalone = abalone_ori[-which.max(abalone_ori$height),]
```


#### The Distribution of Variables

```{r}
par(mfrow = c(2,2))
for (i in colnum_quan) {
  hist(abalone[,i], main = paste('Histogram of', colnames(abalone[i]), sep = ' '), xlab = colnames(abalone[i]), col = 'steelblue')}
```

The distribution of all quantitative variables is right-skewed or left-skewed. `length` and `diameter` are left-skewed. `height`, `whole weight`, `shucked weight`, `viscera weight`, `shell weight` and `rings` are right skewed.



#### Relationship between Variables

```{r}
aggregate(abalone[,-1], list(abalone[,1]), FUN=mean)
```

There is no obvious difference between female and male abalone in all variables, while they have distinct difference with infant abalone.


```{r}
par(mfrow = c(1,2))

n = nrow(abalone)
lbls = names(table(abalone$sex))
pct = round(100*table(abalone$sex)/n)
lab = paste(lbls,":", pct, "%", sep=' ')

pie(table(abalone$sex),labels=lab,col=c('steelblue','beige','coral3'), main='pie chart of sex')

boxplot(abalone$rings ~ abalone$sex,col='steelblue', main = 'box plot of sex and ring', xlab = 'sex', ylab = 'ring') 

```

The proportions of infant, female, and male abalone are roughly same. The average ring of infant abalone is lower than that of female and male abalone, while the average ring and ring range of female and male abalone is nearly the same.


```{r}

panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...) {
  usr <- par("usr")
  on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r <- abs(cor(x, y, use = "complete.obs"))
  txt <- format(c(r, 0.123456789), digits = digits)[1]
  txt <- paste(prefix, txt, sep = "")
  if (missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
  text(0.5, 0.5, txt, cex =  cex.cor * (1 + r) / 2)
}

pairs(abalone[,c(-1)], col = "steelblue", panel = panel.smooth, cex = 0.5,lower.panel = panel.cor, main = "quantitative variables of abalone")
```

We can see that there's no negative relationship between variables. There's obvious positive linear relationship between the four types of weight.`length` also has obvious positive linear relationship with `diameter`. There's strong positive relationship between `height` and `length` or `diameter`, but weak relationship between `height` and the four types of weight.


```{r}
cor(abalone[,-1], method = "pearson",use = "complete.obs")
```

From the correlation matrix, we can see that except `rings`, all other variables has pretty high correlation with each other. Such high correlation coefficients among features can result into multicollinearity. We further investigate multicollinearity later and select variables through examining VIF.


## Linear Regression
From the Exploratory Data Analysis, we can apply linear regression on the dataset.

#### Training Set and Test Set
We select 70% whole dataset as the training set for model building, the other 30% as the test set for model testing. 

```{r}
set.seed(1)
len = dim(abalone)[1]
train_ind = sample(1:len,0.7*len,replace = F)
#cv_ind <- sample(train_ind, 0.4*length(train_ind), replace = F)
train = abalone[train_ind,]
#cv <- abalone[cv_ind,]
test = abalone[-train_ind,]
dim(train)
dim(test)
```


#### Simple Linear Regression Model with All Variables
First of all, we apply simple linear regression model using all variables. 

```{r}
eval_results = function(true, predicted, df) {
  SSE = sum((predicted - true)^2)
  SSTO = sum((true - mean(true))^2)
  R_square = 1 - SSE / SSTO
  RMSE = sqrt(SSE/nrow(df))
  # Model performance metrics
  data.frame(RMSE = RMSE,Rsquare = R_square)
}

fit = lm(rings~., data = train)
summary(fit)

lm.train.pred = predict(fit, newdata = train[,-9]) #Edited to work properly
lm.test.pred = predict(fit, newdata = test[,-9]) #Edited to work properly
eval_results(train[,9], lm.train.pred, train)
eval_results(test[,9], lm.test.pred, test)
```

Based on the given training dataset (`set.seed(1)`) , the $R^2$ is 0.5333, the $R_\alpha^2$ is 0.5318. This model ignores the difference caused by `sex`


```{r}
par(mfrow = c(2,2))
plot(fit)
```

From the residual vs fitted plot, there is a a pattern of moderate heteroscedasticity. In Normal Q-Q plot, the distribution of residuals is heavy tailed. Also, the Scale-Location does not show a straight line, which basically means the full model is not adequate. In the Residual vs Leverage plot, no data point has extreme cook's distance, which shows that there are no extreme influential points in our regression model. 

Now we scale the explanatory variables for further analysis of the remaining data.

```{r}
train.C = train
train.C[,2:8] = scale(train.C[,2:8]) #scale explanatory variables
test.C = test
test.C[,2:8] = scale(test.C[,2:8])

fit.scaled = lm(rings~., data = train.C)
summary(fit.scaled)
```

```{r}

# library(MASS)

boxcoxY <- boxcox(fit.scaled)
lambda <- boxcoxY$x[which.max(boxcoxY$y)]
```

From the Box-Cox plot, transformation of `ring` is needed. When $\lambda=0$, SSE is minimized (or maximum loglikelihood is maximized).

$Y_i^* = (\prod_{j=1}^nY_j)^\frac{1}{n}log(Y_i)$


```{r}
train.C$rings = ((train.C$rings^lambda - 1)/lambda)
test.C$rings = ((test.C$rings^lambda - 1)/lambda)
```

To address heteroscedasticity, we use a box-cox transformation with a slightly negative lambda from the result of our box-cox transformation.

```{r}
train.forVIF <- train.C

fit.vif <- lm(rings~., data = train.forVIF)
testVIF <- as.data.frame(vif(fit.vif))
testVIF

##
while(max(testVIF$GVIF) >= 10)
{
varTBE <- rownames(testVIF[testVIF$GVIF == max(testVIF$GVIF),])
train.forVIF <- train.forVIF[,!names(train.forVIF)%in%varTBE]
fit.vif <- lm(rings~., data = train.forVIF)
testVIF <- as.data.frame(vif(fit.vif))
}

train.C <- train.forVIF
##
```

We attempted to drop the variable with the highest VIF iteratively if the variable with the highest VIF Has a VIF of over 10 or 20, but it ultimately made the models less predictive. Code here is retained for documentation.

```{r}
#train.C <- train.forVIF

fit.scaled.transformed <- lm(rings~., data = train.C)

summary(fit.scaled.transformed)

par(mfrow = c(2,2))
plot(fit.scaled.transformed)
```

The Normal Q-Q plot now looks much closer to normal, but there may still be some non-linearity as seen in the residuals vs fitted plot. We now investigate potential second order models.

```{r, results='hide'}
fitsecond.scaled.transformed = lm(rings~.^2 + I(length^2) + I(diameter^2) + I(height^2) + I(whole_weight^2) + I(shucked_weight^2) + I(viscera_weight^2) + I(shell_weight^2), data=train.C) 

stepAIC(lm(rings~1, data = train.C), scope = list(upper = fitsecond.scaled.transformed, lower = ~1), direction = "both", k = 2)


fitsecond.AIC <- lm(formula = rings ~ diameter + shucked_weight + shell_weight + 
    I(diameter^2) + sex + I(shucked_weight^2) + whole_weight + 
    viscera_weight + height + I(length^2) + shucked_weight:sex + 
    shucked_weight:whole_weight + shucked_weight:height + diameter:whole_weight + 
    shell_weight:whole_weight, data = train.C) 

```
```{r}
summary(fitsecond.scaled.transformed)
summary(fitsecond.AIC)

par(mfrow = c(2,2))
plot(fitsecond.AIC)

fitsecond.train.pred <- predict(fitsecond.AIC, newdata = train.C[,!names(train.C)%in%"rings"]) 
fitsecond.test.pred <- predict(fitsecond.AIC, newdata = test.C[,!names(test.C)%in%"rings"])
eval_results(train.C$rings, fitsecond.train.pred, train.C) 
eval_results(test.C$rings, fitsecond.test.pred, test.C) 

```

Using AIC as our model selection criteria, we arrive at a model that has quite a few second order and interaction terms. However, this may be unstable due to effects from multicollinearity. Despite this, removing variables with high VIF seems to make for a less predictive model. The inclusion of the second-order components and interactions seems to have solved the heteroscedasticity issue. 



# ridge and lasso regression model (categorical variable will be removed)
```{r}
x.train <- as.matrix(train[,-c(1,9)])
y.train <- train[,9]
x.test = as.matrix(test[,-c(1,9)])
y.test = test[,9]
grid <- 10^seq(2, -3, by = -.1)
```

#ridge
```{r}
library(glmnet)
ridge <- glmnet(x.train, y.train, alpha = 0, lambda = grid)
cv.ridge <- cv.glmnet(x.train, y.train, alpha =0, lambda = grid)
opt.lambda.ridge <- cv.ridge$lambda.min
opt.lambda.ridge
```



```{r}
ridge.train.pred <- predict(ridge, s = opt.lambda.ridge, newx = x.train)
ridge.test.pred <- predict(ridge, s = opt.lambda.ridge, newx = x.test)

eval_results(y.train, ridge.train.pred, train[,-1])
eval_results(y.test, ridge.test.pred, x.test[,-1])
```

#lasso
```{r}
lasso <- glmnet(x.train, y.train, alpha = 1, lambda = grid)
cv.lasso <- cv.glmnet(x.train, y.train, alpha =1, lambda = grid)
opt.lambda.lasso <- cv.lasso$lambda.min
opt.lambda.lasso

lasso.train.pred <- predict(lasso, s = opt.lambda.lasso, newx = x.train) 
lasso.test.pred <- predict(lasso, s = opt.lambda.lasso, newx = x.test) 

eval_results(y.train, lasso.train.pred, train[,-1])
eval_results(y.test, lasso.test.pred, x.test[,-1])
```


```{r}
fit1 <- lm(rings~length, data = train)
summary(fit1)
anova(fit1)
```


```{r}
fit2 <- lm(rings~length+whole_weight, data = train)
summary(fit2)
anova(fit2)
```
