---
title: "STA 206 Homework 8"
author: "Kevin Su"
date: "11/25/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Included code

```{r}


#### A simulation to illustrate bias-variance trade-off in regression analysis

## True regression function (mean response): f(x)
## Predictor values (design points): n points equally spaced on [-3,3]
n=30  ##sample size
X=seq(-3,3,length.out=n) ## design points: fixed throughout the simulation 
f.X=sin(X)+sin(2*X)  ## the values of the true regression function on the design points.

par(lwd=2, cex.lab=1.5, cex.main=1.5) ##customize features of the graph in this R session
plot(X, f.X, type='l',  xlab="x", ylab="f(x)", col=1,  main="true regression function") ## look at the true regression function


## Observations: Y_i=f(x_i)+e_i, e_i ~ i.i.d. N(0, sigma.e), i=1,..., n
sigma.e=0.5 ## error standard deviation: consider 0.5, 2, 5
rep=1000 ## number of independent data sets (replicates) to be generated 

Y=matrix (0, n, rep)  ## matrix to record the observations; each column corresponds to one replicate

for (k in 1:rep){
  set.seed(1234+k*56)    ##set seed for the random number generator; for reproducibility of the result 
  e.c=rnorm(n,0,sigma.e) ##generate the random errors
  Y.c=f.X+e.c   ## generate observations for kth replicate: true mean response + error 
  Y[,k]=Y.c
}


## plot the true regression function with the observations for several replicates
## notice the observations are different from replicate to replicate: this is sampling variability 
par(mfrow=c(3,3)) ## create a plot with 3 by 3 panels
for (k in 1:9){
  plot(X, f.X, type='l', xlab="x", ylab="f(x)", ylim=range(Y), col=1, main=paste("rep",k)) ## true regression function; same across replicates
  Y.c=Y[,k]  
  points(X, Y.c)  ## observations of the kth replicate
}

par(mfrow=c(1,1))

## fit polynomial regression models of order l for each replicate; 
## consider l=1 (linear), 2 (quadratic), 3 (cubic), 5, 8,11
## record the fitted values for each replicate
l.order=c(1,2,3,5, 7,9) ## order of the polynomial models to be fitted
Y.fit=array(0, dim=c(n,rep,length(l.order))) ## record the fitted values; 1st index corresponds to cases; 2nd index corresponds to replicates, 3rd index corresponds to models


for (k in 1:rep){
  Y.c=Y[,k] ##observations of the kth replicate
  
  for (l in 1:length(l.order)){
  fit.c=lm(Y.c ~ poly(X, l.order[l], raw=TRUE)) ## fit a polynomial model with order l.order[l]; raw=TRUE means raw polynomial is used; raw= FALSE mean orthogonal polynomial is used
  Y.fit[,k,l]=fitted(fit.c)
  } ## end of l loop
  
}## end of k loop


## plot  the fitted regression curves with observations for several replicates
## notice the fitted response curves are changing from replicate to replicate: this is due to sampling variability
## notice the 8th  and 11th order models tend to overfit the data, while linear and qudratic models tend to underfit. 

label.m=paste(l.order,"order") ## label for each model 
par(mfrow=c(2,2)) ## create a plot with 2 by 2 panels
for (k in 1:4){
  plot(X, f.X, type='l', xlab="x", ylab="f(x)", lwd=2.5, ylim=range(Y[,1:4]),main=paste("rep",k)) ##true regression function (true mean response curve)
  
  Y.c=Y[,k]  
  points(X, Y.c)  ## observations of the kth replicate
  
   for (l in 1:length(l.order)){
    points(X, Y.fit[,k,l], type='l', col=l+1, lty=l+1, lwd=1.5) ## fitted regression function (fitted mean response curve)
   }## end of l loop
  
  legend(x=-1, y=37,legend=c("true", label.m), col=1:(length(l.order)+1), lty=1:(length(l.order)+1), cex=0.5) ## legend for the plot
  
}## end of k loop

par(mfrow=c(1,1))

## examine model bias:  
## compare the average (across replicates) of the fitted response  curves with the true regression function (true mean response)
## notice the 1st order model and 2nd order model both have large biases; but the higher order models have little bias

## examine model variance:
## overlay the fitted response curves  over the true mean response curve 
## notice that the higher order models have larger sampling variability 

Y.fit.ave=apply(Y.fit, c(1,3), mean) ## average across  replicates (2nd index)

par(mfrow=c(3,2))

for (l in 1:length(l.order)){
plot(X, f.X, type='n', xlab="x", ylab="f(x)", ylim=range(Y.fit), main=paste(l.order[l],"order poly model")) ## set plot axis label/limit, title, etc.

 for (k in 1:rep){
    points(X, Y.fit[,k,l], type='l', lwd=1, col=grey(0.6)) ## fitted response curves of lth model: grey
  }## end of k loop

points(X, f.X, type='l',  col=1) ## true mean response: solid black
points(X, Y.fit.ave[,l], type='l', col=2, lty=2) ## averaged (across replicates) fitted mean reponse of the lth model: broken red

legend(x=-0.5,y=40, legend=c("true", "ave.fit"), col=c(1,2), lty=c(1,2)) ## legend of the plot

}##end l loop
par(mfrow=c(1,1))

## compare SSE; variance, bias^2 and mean-squared-estimation-error = variance+bias^2 across models
SSE=matrix(0, rep, length(l.order)) ## record SSE for each model on each replicate
resi=array(0, dim=c(n,rep, length(l.order))) ## record residuals : residual := obs-fitted
error.fit=array(0, dim=c(n,rep, length(l.order))) ## record estimation errors in the fitted values: error := fitted value - true mean response

for (l in 1:length(l.order)){
  temp=Y-Y.fit[,,l]
  resi[,,l]=temp ## residuals
  SSE[,l]=apply(temp^2,2, sum) ## SSE=sum of squared residuals across cases
  error.fit[,,l]=Y.fit[,,l]-matrix(f.X, n, rep, byrow=FALSE) ## estimation error = fitted value - true mean response
}

### in a simulation study, taking average across replicates (i.e., taking empirical mean) is the counterpart of taking mean/expectation of a random variable
### the larger the number of replicates, the closer the empirical mean would be to the actual mean.
SSE.mean=apply(SSE,2,mean) ## mean SSE (averaged over the replicates); this is the empirical version of E(SSE)
bias=apply(error.fit, c(1,3), mean)  ## bias= mean (averaged across replicates) errors in the fitted values
variance=apply(Y.fit, c(1,3), var) ## variance (across replicates) of the fitted values
err2.mean=apply(error.fit^2,c(1,3), mean) ## mean-squared-estimation errors: squared estimation errors of the fitted values averaged across replicates

### compare SSE.mean with (n-l.order-1)*sigma.e^2; What do you find?  
### note: l.order+1 is the number of regression coefficients p in the lth moder
### for a correct model (models with all important X variables included), E(SSE)=(n-p)*sigma^2
### does this hold for an underfit model (models with some important X variables omitted)?
cbind(SSE.mean, (n-l.order-1)*sigma.e^2)



### check err2.mean=bias^2+variance; this only holds approximately, because 1/(rep-1) is used in sample variance calculation
summary(as.vector(abs(err2.mean-(bias^2+variance)))) ## the discrepancy is reasonably small
summary(as.vector(abs(err2.mean-(bias^2+variance*(rep-1)/rep)))) ## no discrepancy 

### bias, variance, err2.mean are calculated on each design point/case for each model
### to facilitate comparison among models, we sum them across the design points/cases to produce an overall quantity (each) for each model
bias2.ave=apply(bias^2, 2, mean) ## average bias^2 across design points  for each model: overall in-sample bias
variance.ave=apply(variance, 2,mean) ## average variance across design points for each model: overall in-sample variance
err2.mean.ave=apply(err2.mean,2, mean) ## average mean-squared-estimation-error across design points for each model: over-all in-sample msee

### compare variance.ave*n/sigma.e^2 with l.order+1. What do you observe? Can you explain it? 
cbind(variance.ave*n/sigma.e^2, l.order+1)

### plot E(SSE), E(MSE), bias^2, variance, mean-squared-estimation-error against the model order to examine bias-variance trade-off
par(mfrow=c(3,2))
plot(l.order, SSE.mean, type='o',xlab="order of model", ylab="E(SSE)", main="E(SSE)")  
points(l.order, sigma.e^2*(n-l.order-1), type='l',lty=2,col=2)


plot(l.order, SSE.mean/(n-l.order-1), type='o',xlab="order of model", ylab="E(MSE)", main="E(MSE)") 
abline(h= sigma.e^2,  lty=2,col=2)
plot(l.order, bias2.ave, type='o',xlab="order of model", ylab="bias^2", main="squared model bias")

plot(l.order, variance.ave, type='o',xlab="order of model", ylab="variance", main="model variance") 
points(l.order, (l.order+1)*sigma.e^2/n, type='l', lty=2,col=2)

plot(l.order, err2.mean.ave, type='o',xlab="order of model", ylab="mean-squared-estimation-error", main="mean-squared-estimation-error")

par(mfrow=c(1,1))

```

# Problem 2

## A
There is no "correct" model among the models being considered as trig functions are not being considered, but the approximations seem to get better with model complexity without increasing variance by too much.

## B

```{r}
variance.ave
```

The model variance increases in an almost linear pace with model complexity. It appears that the error variance still decreases with increasing complexity despite the increase in model variance in this case.

## C

```{r}
bias2.ave
```

The squared model bias drops off steeply as model complexity increases and has nearly bottomed out at the 5th order model though the bias remains relatively high at both first and second order. The decrease in model bias corresponds to the decrease in error variance.

## D

The model bias is the dominant component in mean squared estimation error here. The answer does not depend too much on the true error variance as it is irreducible error.

## E

The best model according to mean-squared estimation error is the model with order 7. This depends on the model variance as at this point the model variance is the dominant component of the mean-squared estimation error.

## F

E[SSE] is also made of components of variance and bias. at lower complexity, bias is the dominant component and at higher complexity variance is the dominant component but it should be monotonically decreasing regardless. 


# Problem 3

## A

```{r}
diabetes <- read.table("C:/Users/kevin/Downloads/diabetes.txt", header = T)
diabetes$frame[diabetes$frame == ""] <- NA
```

## B

```{r}
drops = c("id", "bp.2s", "bp.2d")
data = diabetes[,!(names(diabetes)%in%drops)]
```

## C

```{r}
sapply(data, mode)

hist(data$glyhb, main = "GlyHb")

par(mfrow = c(3,4))
hist(data$chol, main = "Cholesterol")
hist(data$stab.glu, main = "Glucose")
hist(data$hdl, main = "HDL")
hist(data$ratio, main = "Ratio")
hist(data$age, main = "Age")
hist(data$height, main = "Height")
hist(data$weight, main = "Weight")
hist(data$bp.1s, main = "Systolic BP")
hist(data$bp.1d, main = "Diastolic BP")
hist(data$waist, main = "Waist")
hist(data$hip, main = "Hip")
hist(data$time.ppn, main = "time.ppn")

par(mfrow = c(1,3))
pie(table(data$location), main = "Location", labels = paste(names(table(data$location)), table(data$location)))
pie(table(data$gender), main = "Gender", labels = paste(names(table(data$gender)), table(data$gender)))
pie(table(data$frame), main = "Frame", labels = paste(names(table(data$frame)), table(data$frame)))
```

Quantitative Variables: "chol", "stab.glu", "hdl", "ratio", "glyhb", "age", "height", "weight", "bp.1s", "bp.1d", "waist", "hip", "time.ppn"

Qualitative Variables: "location", "gender", "frame"

## D
```{r}
hist(log(data$glyhb), breaks = 15, main = "log of glyhb")     #glyhb*
hist(sqrt(data$glyhb), breaks = 15, main = "sqrt of glyhb")
hist(1/data$glyhb, breaks = 15, main = "1/glyhb")
glyhb = log(data$glyhb) # ****
data$glyhb = glyhb
```

Despite the long right tail, the log of glyhb appears the most Normal like among the three as 1/glyhb has a long thick left tail and the sqrt seems steeper (higher kurtosis)

## E

```{r}
pairs(data[,c(1:5, 7, 9, 10, 12:16)])

cor(data[,c(1:5, 7,9,10, 12:16)], use = "pairwise.complete.obs")
```

There is no obvious nonlinearity except where "ratio" and "hdl" are involved. There may also be some nonlinearity in the interaction between stab.glu and glyhb.

## F

```{r}
glyhbM <- data$glyhb[which(data$gender == "male")]
glyhbF <- data$glyhb[which(data$gender == "female")]

boxplot(glyhbF, glyhbM, names = c("Female", "Male"), main = "glyhb by gender", horizontal = T)

glyhblarge <- data$glyhb[which(data$frame == "large")]
glyhbmedium <- data$glyhb[which(data$frame == "medium")]
glyhbsmall <- data$glyhb[which(data$frame == "small")]

boxplot(glyhbsmall, glyhbmedium, glyhblarge, names = c("Small", "Medium", "Large"), main = "glyhb by frame", horizontal = T)
```

## G

```{r}
set.seed(10)
train.samp <- sample(1:403, 202, replace = F)

train.data <- na.omit(data[train.samp, ])
test.data <- na.omit(data[-train.samp, ])

```

## H

```{r}
boxplot(train.data$glyhb, test.data$glyhb, names = c("Train", "Test"), main = "GlyHb test vs train", horizontal = T)

boxplot(train.data$stab.glu, test.data$stab.glu, names = c("Train", "Test"), main = "Stabilized Glucose test vs train", horizontal = T)

boxplot(train.data$ratio, test.data$ratio, names = c("Train", "Test"), main = "ratio test vs train", horizontal = T)

boxplot(train.data$age, test.data$age, names = c("Train", "Test"), main = "Age test vs train", horizontal = T)

boxplot(train.data$bp.1s, test.data$bp.1s, names = c("Train", "Test"), main = "Systolic BP test vs train", horizontal = T)

boxplot(train.data$waist, test.data$waist, names = c("Train", "Test"), main = "Waist test vs train", horizontal = T)
```

The Training and Test datasets look approximately similar, though Systolic BP seems to have lower variance in the Training dataset.

# Problem 4

## A

```{r}
Model1 <- lm(glyhb~., data = train.data)

anova(Model1)

library(MASS)

boxcox(Model1)
```

There are 16 regression coefficients in this model, including the intercept. The MSE is 0.0435.

As the box-cox procedure yields a lambda whose 95% confidence interval does not include 1, a further transformation of the response variable is still needed.

## B

```{r}
library("leaps")

subs <- regsubsets(glyhb~., data = train.data, nbest = 1, nvmax = 16, method = "exhaustive")

subselect <- summary(subs)

n = nrow(train.data)

p <- rowSums(subselect$which)
sst <- sum((train.data$glyhb-mean(train.data$glyhb, na.rm = T))^2, na.rm = T)
subssse <- (1-subselect$rsq)*sst

aic <- n*log(subssse/n) + 2*p
bic <- n*log(subssse/n) + log(n)*p

bsubs <- cbind(subselect$which, subssse, subselect$rsq, subselect$adjr2, subselect$cp, aic, bic)
colnames(bsubs) <- c(colnames(subselect$which), "sse", "Rsq", "Rsq-adj", "Cp", "AIC", "BIC")
round(bsubs, digits = 3)

```


The Mallows Cp criterion here is very close to 0. Here, the bias and variance is very small such that in thie case Cp < p. 

## C

```{r}
NullModel <- lm(glyhb~1, data = train.data) #having trouble with NA when running stepAIC
FullModel <- lm(glyhb~., data = train.data)

stepAIC(NullModel, scope = list(upper = FullModel, lower = ~1), direction = "both", k = 2)

Modelfs1 <- lm(formula = glyhb ~ stab.glu + chol + location + ratio + age + time.ppn, data = train.data)
```

The model is identical to the model selected in the previous question. 

## D

```{r}
summary(Modelfs1)

par(mfrow = c(1,2))
plot(Modelfs1, which = c(1,2))
```


The residual vs fitted plot does not seem unusual. The distribution of the residual QQ plot seems a bit heavy-failed with observations 195 and 334 appearing as notable outliers.

# Problem 5

## A

```{r}
Model2 <- lm(glyhb~.^2, data = train.data)
Model2Sums <- summary(Model2)

length(Model2$coefficients)
anova(Model2)
```

There are 136 coefficients in this model. The MSE of this model is 0.0313. The fitting of this model is clearly not ideal as not all interaction terms are significant.

## B

```{r}
stepAIC(NullModel, scope = list(upper = Model2, lower = ~1), direction = "both", k = 2)

Modelfs2 <- lm(formula = glyhb ~ stab.glu + chol + location + time.ppn + ratio + age + stab.glu:location + location:time.ppn + time.ppn:age + chol:age + stab.glu:age + location:ratio + chol:ratio, data = train.data)
  
```

The Model being selected is 

lm(formula = glyhb ~ stab.glu + chol + location + time.ppn + 
    ratio + age + stab.glu:location + location:time.ppn + time.ppn:age + 
    chol:age + stab.glu:age + location:ratio + chol:ratio, data = train.data)
    
and the AIC value is -573.2, an improvement over the model from problem 4.

## C

```{r}
summary(Modelfs2)

par(mfrow = c(1,2))
plot(Modelfs2, which = c(1,2))
```
The residual vs fitted and residual normal qq plot of Model fs2 seems to be mostly similar to Model fs1, with observations 195 and 334 being the most extreme values and the QQ plot indicating a heavy-tailed distribution. 

## D

```{r}
stepAIC(NullModel, scope = list(upper = Model2, lower = ~1), direction = "forward", k = 2)
```

Here we end up with the model 

lm(formula = glyhb ~ stab.glu + chol + location + time.ppn + 
    ratio + age + stab.glu:location + stab.glu:time.ppn + location:time.ppn + 
    time.ppn:age + chol:age + stab.glu:age + location:ratio, 
    data = train.data)

and has a corresponding AIC of -572.12.

